Topic : Mining Top-K tweets which cover the maximum number of aspects/best describe the topic

Dataset : Currently I am working with 20k Topic-specified tweets for each topic, 4 different topics( Extracting tweets is taking time due to rate limits, as days progress I will have more data)

My work/thought process :

We have two tasks :
	1. Classify which aspects are relevant to topic
	2. Find an algo to mine Top-K tweets which cover the maximum number of aspects
	3. Sentiment Analysis on the mined tweets for info on features

First we clean the data by removing any repetitive tweets. We do not want such kind of tweets as while they might show popularity of a tweet, they do not provide us any new information about the topic. I also removed the tweets which were just URLs(no words or emojis) as they provide no info on sentiment.

Aspect Mining and Classification - DONE EARLIER

I also got the pmi feature working using Yahoo :)

Current Threshold values :(We need to think more on this )

An aspect should be mentioned atleast twice in the whole dataset to be considered an aspect
Reasoning : Two is low a threshold value for 20k large dataset so even rarely mentioned features can be included.Plus I have merged all the aspects like Barack_Obama,Barack,Obama,OBAMA,@Obama,#Obama before taking the count. 
Should we have a larger threshold value for repetitions ?

Aspect-Topic PMI should be +ve 

ALGORITHM to mine top -k tweets best describing aspects:


What I noticed was n/k remained constant algorithm-wise
n- no. of aspects covered in top k tweets

NOTE : Generally a tweet has about 5-6 aspects which are also not tweetwise independant.So n/k < 5(upper bound) 

1. Greedy algorithm :  at each stage, choose the tweet that contains the largest number of aspects.
n/k ~ 2-2.5
i.e. top 100 tweets give you cover around ~240 aspects
doesnt work well when the dataset total number of aspects increase correspondingly(~300 aspects for 2.5k tweets to ~700 aspects for ~30k tweets) and hence the % of aspects covered is significantly less(dropping from 80% to 50%)

2. Greedy algorithm(improved):  at each stage, choose the tweet that contains the largest number of uncovered aspects.
n/k ~ 3-3.5
i.e. top 100 tweets give you cover around ~330 aspects

3. Graphwise, Aspects- Tweets is a bipartite graph( If you draw an edge between an aspect A and tweet T, if A is an aspect of T )
   And this is a problem of optimizing minimum vertex cover in the bipartite graph.
   In bipartite graphs, cardinality(minimum vertex cover)= cardinaity(maximum matching) and minimum vertex cover and maximum matching will always exist.
cardinality(minimum vertex cover) here is same as min. no of tweets required which can cover all possible aspects which can be covered
   So I implemeted Ford Fulkerson algorithm for maximum matching and also another for minimum vertex cover(just for reassurance)
 because whatever percentage of aspects these two will cover that will always be maximumum % of aspects covered.
	So, I suggest that we rate the efficiency of our algorithm is not on basis of % aspects covered but on the ratio (% aspects covered by algo/% of aspects covered by minimum vertex cover algo) which is ~75-80% for greedy(improved) if k=100

This is what I have done till date. Please tell me if you have suggestions for any better algo.

SENTIMENT ANALYSIS :

I was thinking we could add another little part here where we find % of aspects where polarity of aspect over whole dataset is same as polarity over extracted top-k tweets.. to show accuracy.
 	
		
	

3. This is 
